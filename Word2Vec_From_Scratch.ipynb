{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erfan-kalandi/Word2Vec-From-Scratch/blob/main/Word2Vec_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UL3KIKrzvjcd"
      },
      "source": [
        "### Word2Vec Implementation from Scratch\n",
        "\n",
        " Word2Vec is a popular technique for word embeddings, which captures the meaning of words by placing them in a continuous vector space.\n",
        " In this code, we will implement Word2Vec using NumPy.\n",
        "We will represent each word as a one-hot vector, meaning each word in the vocabulary is mapped to a unique binary vector with only one active (1) position."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5uKnkUJvJDq",
        "jupyter": {
          "is_executing": true
        }
      },
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dzazztv_wtbI"
      },
      "outputs": [],
      "source": [
        "settings = {\n",
        "\t'window_size': 2,\n",
        "\t'n': 10,\n",
        "\t'epochs': 200,\n",
        "\t'learning_rate': 0.01\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlpwAgpnv6tY"
      },
      "outputs": [],
      "source": [
        "class word2vec:\n",
        "    def __init__(self, settings):\n",
        "        self.n = settings['n']\n",
        "        self.lr = settings['learning_rate']\n",
        "        self.epochs = settings['epochs']\n",
        "        self.window = settings['window_size']\n",
        "\n",
        "    def generate_training_data(self, corpus):\n",
        "        word_counts = defaultdict(int)\n",
        "        for sentence in corpus:\n",
        "            for word in sentence:\n",
        "                word_counts[word] += 1\n",
        "\n",
        "        self.v_count = len(word_counts.keys())\n",
        "\n",
        "        self.words_list = list(word_counts.keys())\n",
        "\n",
        "        self.word_index = {}\n",
        "        self.index_word = {}\n",
        "        for i, word in enumerate(self.words_list):\n",
        "          self.word_index[word] = i\n",
        "          self.index_word[i] = word\n",
        "\n",
        "        training_data = []\n",
        "        for line in corpus:\n",
        "          for i in range(len(line)):\n",
        "              target = sentence[i]\n",
        "              context = [\n",
        "                  self.word2onehot(sentence[j])\n",
        "                  for j in range(i - self.window, i + self.window)\n",
        "                  if i != j and 0 <= j < len(sentence)\n",
        "              ]\n",
        "          training_data.append([self.word2onehot(target),  np.array(context)])\n",
        "\n",
        "        return training_data\n",
        "\n",
        "    def word2onehot(self, word):\n",
        "        word_vec = np.zeros(self.v_count)\n",
        "        word_vec[self.word_index[word]] = 1\n",
        "        return word_vec\n",
        "\n",
        "    def train(self, training_data):\n",
        "        self.w1 = np.random.uniform(-1, 1, (self.v_count, self.n))\n",
        "        self.w2 = np.random.uniform(-1, 1, (self.n, self.v_count))\n",
        "        for i in range(self.epochs):\n",
        "            self.loss = 0\n",
        "            for target, context in training_data:\n",
        "              x = target\n",
        "              y_c, h, u = self.forward_pass(target)\n",
        "              error = np.sum([np.subtract(y_c, word) for word in context], axis=0)\n",
        "\n",
        "              self.backprop(error, h, target)\n",
        "\n",
        "              loss_for_context = 0\n",
        "              for word in context:\n",
        "                  true_word_index = np.argmax(word)\n",
        "                  loss_for_context += -np.log(y_c[true_word_index])\n",
        "              self.loss += loss_for_context\n",
        "\n",
        "            print('Epoch:', i, \"Loss:\", self.loss)\n",
        "\n",
        "    def softmax(self, x):\n",
        "        e_x = np.exp(x - np.max(x))\n",
        "        return e_x / e_x.sum(axis=0)\n",
        "\n",
        "    def forward_pass(self, x):\n",
        "        h = np.dot(x, self.w1)\n",
        "        u = np.dot(h, self.w2)\n",
        "        y_c = self.softmax(u)\n",
        "\n",
        "        return y_c, h, u\n",
        "\n",
        "    def backprop(self, e, h, x):\n",
        "        dl_dw2 = np.outer(h, e)\n",
        "        dl_dw1 = np.outer(x, np.dot(self.w2, e.T))\n",
        "        self.w1 = self.w1 - (self.lr * dl_dw1)\n",
        "        self.w2 = self.w2 - (self.lr * dl_dw2)\n",
        "\n",
        "    def word_vec(self, word):\n",
        "        w_index = self.word_index[word]\n",
        "        v_w = self.w1[w_index]\n",
        "        return v_w\n",
        "\n",
        "    def vec_sim(self, word, top_n):\n",
        "        v_w1 = self.word_vec(word)\n",
        "        word_sim = {}\n",
        "\n",
        "        for i in range(self.v_count):\n",
        "          v_w2 = self.w1[i]\n",
        "          theta_sum = np.dot(v_w1, v_w2)\n",
        "          theta_den = np.linalg.norm(v_w1) * np.linalg.norm(v_w2)\n",
        "          theta = theta_sum / theta_den\n",
        "\n",
        "          word = self.index_word[i]\n",
        "          word_sim[word] = theta\n",
        "\n",
        "        words_sorted = sorted(word_sim.items(), key=lambda kv: kv[1], reverse=True)\n",
        "\n",
        "        for word, sim in words_sorted[:top_n]:\n",
        "          print(word, sim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrARMd5i6Zqt"
      },
      "outputs": [],
      "source": [
        "text = \"Natural language processing and machine learning open up fascinating possibilities, allowing machines to analyze,\\\n",
        " understand, and respond to human language in ways that were once thought impossible.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7ntj1Es6u2x"
      },
      "outputs": [],
      "source": [
        "corpus = [[word.lower() for word in text.split()]]\n",
        "\n",
        "w2v = word2vec(settings)\n",
        "\n",
        "training_data = w2v.generate_training_data(corpus)\n",
        "\n",
        "w2v.train(training_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPbTmdZu6uJA"
      },
      "outputs": [],
      "source": [
        "word = \"machine\"\n",
        "vec = w2v.word_vec(word)\n",
        "print(word, vec)\n",
        "\n",
        "w2v.vec_sim(\"machine\", 5)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}